{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a7003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8d9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_list = ['audi', 'skoda', 'honda', 'volvo', 'bmw', 'nissan',\n",
    "              'infiniti', 'toyota', 'lexus', 'volkswagen', 'mitsubishi', 'mercedes'] #список брендов из тестового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем цикл парсинга для каждого бренда из списка\n",
    "for b in brand_list:\n",
    "    \n",
    "    headers={'User-Agent': 'Mozilla/5.0'}\n",
    "    # получаем начальную страницу для всех объявлений бренда\n",
    "    response = requests.get('https://auto.ru/cars/'+b+'/used/', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # находим сколько всего страниц с объявлениями данного бренда\n",
    "    amount_pages = int(soup.find_all('div', class_='ListingCarsPagination')[0].find_all('a', class_='Button')[-3].get_text())\n",
    "    # определим класс для удобства записи и хранения некоторой информации об авто\n",
    "    class CarCard():\n",
    "        pass\n",
    "\n",
    "    # запускаем цикл перебора страниц объявлений для конкретного бренда\n",
    "    for n in range(0, amount_pages):\n",
    "        links_to_cars = [] # создадим список в котором будут храниться ссылки на все авто данной страницы\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        list_audi = soup.find_all('div', class_='ListingItem__main') # получаем список из объявлений на текущей странице цикла\n",
    "        car_card = CarCard()\n",
    "    \n",
    "        # запускаем цикл по всем объявлениям на текущей странице\n",
    "        for i in range(0, len(list_audi)):\n",
    "            car_card.url = list_audi[i].find('a', class_='ListingItemTitle__link')['href'] # находим на странице ссылку на каждое авто\n",
    "            # находим на странице цену для каждого авто\n",
    "            car_card.price = list_audi[i].find('div', class_='ListingItemPrice__content').get_text().replace('\\xa0', ' ') if list_audi[i].find('div', class_='ListingItemPrice__content') is not None else 'sold'\n",
    "            links_to_cars.append([car_card.url, car_card.price]) # добавляем в ранее определенный список ссылку на авто и его цену\n",
    "        # запускаем цикл со ссылками на авто текущей страницы\n",
    "        for f,s in links_to_cars:\n",
    "            print('start car', f)\n",
    "            during_car_dict = {} # создаем словарь в который будем записывать всю информацию по текущему авто \n",
    "            res = requests.get(f, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            res.encoding = 'utf-8'\n",
    "            car = BeautifulSoup(res.text, 'html.parser')\n",
    "            if 'Ошибка' in car.title.text: # пропускаем текущее авто если страница не открылась\n",
    "                continue\n",
    "            information = car.find('ul', class_='CardInfo') # получаем основную информацию об авто из объявления\n",
    "            link_to_catalog = car.find('a', class_='CardCatalogLink') # получаем ссылку на каталог данного авто для доп. информации, если это возможно\n",
    "            if link_to_catalog is not None:\n",
    "                link_to_catalog = link_to_catalog['href']\n",
    "                res_catalog = requests.get(link_to_catalog, headers=headers)\n",
    "                res_catalog.encoding = 'utf-8'\n",
    "                catalog_inf = BeautifulSoup(res_catalog.text, 'html.parser')\n",
    "                # если удается открыть каталог, получаем информацию о кол-ве дверей в авто и годах выпуска данной модели\n",
    "                if 'Ошибка' not in catalog_inf.title.text:\n",
    "                    doors = catalog_inf.find('dt', class_='list-values__label', string='Количество дверей')\n",
    "                    doors_amount = int(doors.find_next_sibling('dd').text)\n",
    "                    model_year = catalog_inf.find('a', class_='search-form-v2-mmm__breadcrumbs-item_type_generation').text\n",
    "                    during_car_dict['doors_amount'] = doors_amount\n",
    "                    during_car_dict['model_year'] = model_year\n",
    "            during_car_dict['model'] = car.find_all('h1', class_='CardHead__title')[0].get_text()\n",
    "            during_car_dict['url'] = f\n",
    "            during_car_dict['price'] = s\n",
    "            during_car_dict['parsing_unixtime'] = round(time.time())\n",
    "        \n",
    "            for i in information:\n",
    "                test_param = i.get_text(',').replace('\\xa0', ' ').split(',', 1)\n",
    "                during_car_dict[test_param[0]] = test_param[1]\n",
    "            \n",
    "            # создаем датасет с характеристиками одного авто и объединяем с существующим датасетом\n",
    "            mini_df = pd.DataFrame(data=during_car_dict, index=range(0,1)) \n",
    "            df = pd.concat([df, mini_df], ignore_index=True)    \n",
    "    \n",
    "        if n == amount_pages:\n",
    "            continue\n",
    "        # получаем ссылку на следующую страницу с объявлениями и продолжаем цикл\n",
    "        next_page = soup.find_all('div', class_='ListingCarsPagination')[0].find_all('a', class_='Button')[-1]['href']\n",
    "        response = requests.get(next_page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea8834",
   "metadata": {},
   "source": [
    "## Далее немного обработаем данные полученые при парсинге, и сразу выделим из них пару отдельных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'] = df['model'].apply(lambda x: x.split(' ')[0])\n",
    "df['Двигатель'] = df['Двигатель'].apply(lambda x: x.split('/'))\n",
    "df['fuel_type'] = df['Двигатель'].apply(lambda x: x[2][2:])\n",
    "df['fuel_type'] = df['fuel_type'].apply(lambda x: 'Электро' if x=='лектро' else x)\n",
    "df['enginePower'] = df['Двигатель'].apply(lambda x: x[1])\n",
    "df['engineVolume'] = df['Двигатель'].apply(lambda x: x[0])\n",
    "pattern = re.compile('20\\d\\d|19\\d\\d')\n",
    "df['modelDate'] = df['model_year'].apply(lambda x: pattern.findall(x)[0]if type(x) is str else x)\n",
    "df['modelDate'] = df['modelDate'].apply(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884eb423",
   "metadata": {},
   "source": [
    "## Записываем получившийся датасет в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_auto.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

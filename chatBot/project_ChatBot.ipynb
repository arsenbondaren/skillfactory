{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55b5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import annoy\n",
    "import codecs\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import bz2file as bz2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d358b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def preprocess_txt(line: str):\n",
    "    \"\"\"Функция, предобрабатывающая текст. На вход принимает строку текста, возвращает список обработаных слов\"\"\"\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74712a",
   "metadata": {},
   "source": [
    "## Подгрузка имеющихся моделей и индексов\n",
    "В ячейке ниже задокументирован блок кода из прошлого задания, в котором обучена модель на вопросах из mail.ru, создан annoy индекс по вопросам и словарь с ответами по индексам. Всё сохранено в файлы. Далее в функциях будут использованы загруженные из этих файлов модели и индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8ebea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62c70ea3e874655aebbb7ce8dd87fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = None\n",
    "written = False\n",
    "\n",
    "#Мы идем по всем записям, берем первую строку как вопрос\n",
    "# и после знака --- находим ответ\n",
    "with codecs.open(\"prepared_answers.txt\",\"w\", \"utf-8\") as fout:\n",
    "    with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            if line.startswith(\"---\"):\n",
    "                written = False\n",
    "                continue\n",
    "            if not written and question is not None:\n",
    "                fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                written = True\n",
    "                question = None\n",
    "                continue\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c690651a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c850c6e51e84486ad44231b05336332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = []\n",
    "c = 0\n",
    "\n",
    "with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as fin:\n",
    "    for line in tqdm(fin):\n",
    "        spls = preprocess_txt(line)\n",
    "        sentences.append(spls)\n",
    "        c += 1\n",
    "        if c > 300000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0f0321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим модель word2vec на наших вопросах\n",
    "sentences = [i for i in sentences if len(i) > 2]\n",
    "model_speaker = Word2Vec(sentences=sentences, vector_size=50, min_count=3, window=5)\n",
    "model_speaker.save(\"w2v_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fdb3ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ecc92738d945328bd67fa0e1c55a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = annoy.AnnoyIndex(50 ,'angular')\n",
    "\n",
    "index_map = {}\n",
    "counter = 0\n",
    "\n",
    "with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        n_w2v = 0\n",
    "        spls = line.split(\"\\t\")\n",
    "        index_map[counter] = spls[1]\n",
    "        question = preprocess_txt(spls[0])\n",
    "        vector = np.zeros(50)\n",
    "        for word in question:\n",
    "            if word in model_speaker.wv:\n",
    "                vector += model_speaker.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        index.add_item(counter, vector)\n",
    "            \n",
    "        counter += 1\n",
    "        if counter>300000:\n",
    "            break\n",
    "\n",
    "index.build(6)\n",
    "index.save('speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44a05a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним словарь с ответами в файл\n",
    "with open('dict_for_speaker.pkl', 'wb') as file:\n",
    "     pickle.dump(index_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531c9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = None\n",
    "written = False\n",
    "\n",
    "#Мы идем по всем записям, берем первую строку как вопрос\n",
    "# и после знака --- находим ответ\n",
    "with codecs.open(\"prepared_answers.txt\",\"w\", \"utf-8\") as fout:\n",
    "    with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            if line.startswith(\"---\"):\n",
    "                written = False\n",
    "                continue\n",
    "            if not written and question is not None:\n",
    "                fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                written = True\n",
    "                question = None\n",
    "                continue\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue\n",
    "                \n",
    "sentences = []\n",
    "c = 0\n",
    "\n",
    "with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as fin:\n",
    "    for line in tqdm(fin):\n",
    "        spls = preprocess_txt(line)\n",
    "        sentences.append(spls)\n",
    "        c += 1\n",
    "        if c > 500000:\n",
    "            break\n",
    "            \n",
    "# Обучим модель word2vec на наших вопросах\n",
    "sentences = [i for i in sentences if len(i) > 2]\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, min_count=1, window=5)\n",
    "model.save(\"w2v_model\")\n",
    "\n",
    "index = annoy.AnnoyIndex(100 ,'angular')\n",
    "\n",
    "index_map = {}\n",
    "counter = 0\n",
    "\n",
    "with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        n_w2v = 0\n",
    "        spls = line.split(\"\\t\")\n",
    "        index_map[counter] = spls[1]\n",
    "        question = preprocess_txt(spls[0])\n",
    "        vector = np.zeros(100)\n",
    "        for word in question:\n",
    "            if word in model_speaker.wv:\n",
    "                vector += model_speaker.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        index.add_item(counter, vector)\n",
    "            \n",
    "        counter += 1\n",
    "\n",
    "index.build(10)\n",
    "index.save('speaker.ann')\n",
    "\n",
    "# Сохраним словарь с ответами в файл\n",
    "with open('dict_for_speaker.pkl', 'wb') as file:\n",
    "     pickle.dump(index_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0563566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем из файлов словарь для чат-бота, модель и индексы. Будем их использовать в функции speaker_answer\n",
    "index_map = pickle.load(open('dict_for_speaker.pkl', 'rb'))\n",
    "#model_speaker = KeyedVectors.load('w2v_model')\n",
    "index_speaker = annoy.AnnoyIndex(100 ,'angular')\n",
    "index_speaker.load('speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41b69fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_speaker = Word2Vec.load('w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b5c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle a file and then compress it into a file with extension \n",
    "def compressed_pickle(title, data):\n",
    "    with bz2.BZ2File(title + '.pbz2', 'wb') as f: \n",
    "        pickle.dump(data, f, protocol=5)\n",
    "    \n",
    "    \n",
    "# Load any compressed pickle file\n",
    "def decompress_pickle(file):\n",
    "    data = bz2.BZ2File(file, 'rb')\n",
    "    data = pickle.load(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d37a7ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_speaker = annoy.AnnoyIndex(50 ,'angular')\n",
    "index_speaker.load('speaker.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6edc68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "824caaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_pickle('dict_for_speaker.pkl', index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba8b43",
   "metadata": {},
   "source": [
    "## Обработка новых данных и обучение с их помощью классификатора\n",
    "Для обучения классификатора создаем отдельный датафрейм, в котором данные из готовых вопросов mail.ru помечаем 0, а названия товаров 1. Все данные векторизуем с помощью HashingVectorizer и классифицируем с помощью LogisticRegression. Классификатор и векторизаторы сохраним в файлы, для последующего применения в функциях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378b7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим датафрейм с товарами\n",
    "df = pd.read_csv('ProductsDataset.csv').drop(columns=['properties', 'image_links'])\n",
    "df['descrirption'] = df['descrirption'].fillna(' ')\n",
    "df['item'] = df['title'] + '. ' + df['descrirption']\n",
    "#df['item'] = df['item'].apply(preprocess_txt)\n",
    "#df['item'] = df['item'].apply(lambda x: [i for i in x if len(i) > 2])\n",
    "df['preproc_title'] = df['title'].apply(preprocess_txt)\n",
    "df['preproc_title'] = df['preproc_title'].apply(lambda x: ','.join(x))\n",
    "df.to_csv('ready_ProductsDataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b047ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba54e583d99442f8faba0428c7240ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Подготовим список вопросов c mail.ru для обучения классификатора и сохраним в файл\n",
    "questions = []\n",
    "count = 0\n",
    "with codecs.open(\"prepared_answers.txt\", \"r\", \"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        question = line.split(\"\\t\")[0]\n",
    "        processed_question = ','.join(preprocess_txt(question))\n",
    "        questions.append(processed_question)\n",
    "        count += 1\n",
    "        if count > 40000:\n",
    "            break\n",
    "            \n",
    "for i, v in enumerate(questions):\n",
    "    if not v:\n",
    "        questions.pop(i)\n",
    "        \n",
    "pickle.dump(questions, open('questions_list_clf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc76be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pickle.load(open('questions_list_clf.pkl', 'rb'))\n",
    "df = pd.read_csv('ready_ProductsDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13005e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a632d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим датафрейм для обучения классификатора. Товарные запросы помечаем 1, все остальные 0\n",
    "df_classif = pd.DataFrame(columns = ['query', 'mark'])\n",
    "df_classif['query'] = questions[:35500]\n",
    "df_classif['mark'] = 0\n",
    "\n",
    "df_classif2 = pd.DataFrame(columns = ['query', 'mark'])\n",
    "df_classif2['query'] = df['preproc_title']\n",
    "df_classif2['mark'] = 1\n",
    "\n",
    "df_clf = pd.concat([df_classif, df_classif2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c09d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8968818188217076\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6974,  106],\n",
       "       [1359, 5768]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим и обучим классификатор, определяющий относится ли запрос к нашим товарам или просто \"болталка\"\n",
    "# Векторизуем текст с помощью HashingVectorizer и потом преобразуем в обычный numpy вектор с TruncatedSVD\n",
    "# Для классификации используем логистическую регрессию\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train, test = train_test_split(df_clf, test_size = 0.2)\n",
    "\n",
    "vectorizer_clf = HashingVectorizer(n_features=2**16, ngram_range=(1,1))\n",
    "train_values = vectorizer_clf.fit_transform(train['query'])\n",
    "pickle.dump(vectorizer_clf, open('hash_vectorizer.pkl', 'wb'))\n",
    "\n",
    "svd_clf = TruncatedSVD(n_components=100, random_state=10)\n",
    "train_X = svd_clf.fit_transform(train_values)\n",
    "pickle.dump(svd_clf, open('svd_transformer.pkl', 'wb'))\n",
    "\n",
    "test_values = vectorizer_clf.transform(test['query'])\n",
    "test_X = svd_clf.transform(test_values)\n",
    "\n",
    "train_y = train['mark'].values\n",
    "test_y = test['mark'].values\n",
    "\n",
    "model_clf = LogisticRegression(random_state=10).fit(train_X, train_y)\n",
    "predicted_y = model_clf.predict(test_X)\n",
    "pickle.dump(model_clf, open('model_clf.pkl', 'wb'))\n",
    "print('Accuracy score: ', model_clf.score(test_X, test_y))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(test_y, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3dfed",
   "metadata": {},
   "source": [
    "## Поиск подходящего товара по запросу\n",
    "Для поиска подходящего товара(товаров) будем использовать также векторизатор HashingVectorizer, т.к. в продуктовых запросах мало контекстных слов, векторизовать с word2vec тут было бы менее эффективно. Сам поиск реализуем с помощью annoy. Обученные векторизаторы и индекс сохраняем в файлы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9137c9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Поиск подходящего товара по запросу реализуем с помощью алгоритма Annoy.\n",
    "# Для векторизации текста используем HashingVectorizer и TruncatedSVD\n",
    "\n",
    "item_vectorizer = HashingVectorizer(n_features=2**13, ngram_range=(1,1))\n",
    "item_values = item_vectorizer.fit_transform(df['preproc_title'])\n",
    "pickle.dump(item_vectorizer, open('item_vectorizer.pkl', 'wb'))\n",
    "\n",
    "item_svd = TruncatedSVD(n_components=200, random_state=10)\n",
    "items = item_svd.fit_transform(item_values)\n",
    "pickle.dump(item_svd, open('item_transformer.pkl', 'wb'))\n",
    "\n",
    "item_index = annoy.AnnoyIndex(200 ,'angular')\n",
    "\n",
    "for i, b in enumerate(items):\n",
    "    item_index.add_item(i, b)\n",
    "    \n",
    "item_index.build(10)\n",
    "item_index.save('goods_by_title_hash.ann')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96285cc1",
   "metadata": {},
   "source": [
    "## Реализация функции get_answer().\n",
    "Логику работы чат-бота реализуем в функции get_answer. Она в себя включает 3 отдельных функции: классификатор, поиск товара и поиск ответа из разговорного жанра. На вход функция get_answer принимает запрос в формате строки и вторым аргументом, опционально, количество товаров для поиска. По умолчанию она находит один товар, либо дает один ответ на вопрос. При указании числа больше 1, функция вернет указаное число найденных товаров по запросу в формате датафрейма, либо один ответ на вопрос, если расценит запрос не как товарный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b77f6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгружаем все наши обученые выше и сохраненные алгоритмы и индексы\n",
    "\n",
    "# Для функции find_item\n",
    "item_index = annoy.AnnoyIndex(200 ,'angular')\n",
    "item_index.load('goods_by_title_hash.ann')\n",
    "item_vectorizer = pickle.load(open('item_vectorizer.pkl', 'rb'))\n",
    "item_svd = pickle.load(open('item_transformer.pkl', 'rb'))\n",
    "df = pd.read_csv('ready_ProductsDataset.csv')\n",
    "\n",
    "# Для функции classificator\n",
    "vectorizer_clf = pickle.load(open('hash_vectorizer.pkl', 'rb'))\n",
    "svd_clf = pickle.load(open('svd_transformer.pkl', 'rb'))\n",
    "model_clf = pickle.load(open('model_clf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0098d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_item(question, number):\n",
    "    \"\"\"\n",
    "    Функция поиска товара из датасета с продуктами. Принимает вопрос в формате строки и кол-во\n",
    "    товаров для поиска вторым аргументом. Возвращает строку при поиске одного товара, либо\n",
    "    датафрейм при поиске более одного товара\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessed_question = [','.join(preprocess_txt(question))]\n",
    "    vector = item_vectorizer.transform(preprocessed_question)\n",
    "    vector = item_svd.transform(vector).reshape(-1)\n",
    "    index = item_index.get_nns_by_vector(vector, number)\n",
    "    if number==1:\n",
    "        prod_id = df['product_id'].iloc[index].values\n",
    "        prod_id = ''.join(list(prod_id))\n",
    "        title = df['title'].iloc[index].values\n",
    "        title = ''.join(list(title))\n",
    "        return prod_id + \" \" + title\n",
    "    if number > 1:\n",
    "        return df[['product_id', 'title']].iloc[index]\n",
    "    else:\n",
    "        return 'Неверно указано количество товаров. Должно быть целое число больше 0.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c45286ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificator(text):\n",
    "    \"\"\"\n",
    "    Функция, классифицирующая запрос на 'продуктовый' либо 'другой'.\n",
    "    Принимает на вход вопрос в формате строки. \n",
    "    Возврщает 1 для продуктового запроса, либо 0 для всех остальных\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer_clf = pickle.load(open('hash_vectorizer.pkl', 'rb'))\n",
    "    svd_clf = pickle.load(open('svd_transformer.pkl', 'rb'))\n",
    "    model_clf = pickle.load(open('model_clf.pkl', 'rb'))\n",
    "    text = ','.join(preprocess_txt(text))\n",
    "    text = vectorizer_clf.transform([text])\n",
    "    text = svd_clf.transform(text)\n",
    "    text_mark = model_clf.predict(text)\n",
    "    return text_mark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "057e5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_answer(question):\n",
    "    \"\"\"\n",
    "    Функция для поиска ответа из заготовленых с сайта mail.ru.\n",
    "    На вход принимает вопрос в формате строки. Возвращает ответ в формате строки.\n",
    "    В работе использует модель и индекс обученные и сохраненные уроком ранее, и загруженые в начале блокнота\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocessed_question = preprocess_txt(question)\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(50)\n",
    "    for word in preprocessed_question:\n",
    "        if word in model_speaker.wv:\n",
    "            vector += model_speaker.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    answer_index = index_speaker.get_nns_by_vector(vector, 1)\n",
    "    return index_map[answer_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7654164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, number=1):\n",
    "    \"\"\"\n",
    "    Функция, дающая ответ по запросу. Принимает запрос в формате строки и число.\n",
    "    В зависимости от классификации запроса возвращает: ответ в формате строки, для непродуктового запроса;\n",
    "    id товара и его название в формате строки, для продуктового запроса с числом 1;\n",
    "    датафрейм с товарами при продуктовом запросе и числе больше 1.\n",
    "    Если не указать число при запросе, по умолчанию стоит 1.\n",
    "    Если запрос классифицируется как непродуктовый, то указанное число не имеет значения.\n",
    "    \"\"\"\n",
    "    mark = classificator(question)\n",
    "    if mark==1:\n",
    "        return find_item(question, number)\n",
    "    else:\n",
    "        return speaker_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9469e12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.11050695e-05  2.72745200e-04  1.04215967e-04 -3.73541623e-05\n",
      "   1.38280136e-04  3.90471444e-04  4.23034361e-05  2.52482067e-04\n",
      "  -6.32067193e-04  6.51667594e-04  1.30398731e-03 -1.61155261e-03\n",
      "  -2.13432318e-03  9.53850638e-03 -9.55484525e-05 -2.31221734e-05\n",
      "  -6.87750259e-04 -1.26184370e-03  1.89653528e-04 -1.89369278e-04\n",
      "   8.20258371e-04  1.12288399e-04  1.13469959e-03  4.13062088e-04\n",
      "   8.87835399e-04  1.12627823e-03  1.38333537e-03  2.83293947e-02\n",
      "  -4.40114796e-03  5.40248737e-03  2.70266601e-04 -6.85659030e-03\n",
      "  -9.11316664e-06  1.55768666e-03 -1.16994497e-02 -1.03249204e-03\n",
      "   1.77475407e-03 -1.70051091e-03  1.00702291e-03  1.37115899e-03\n",
      "  -2.25566854e-04  9.45436173e-05 -1.10175061e-03  1.61184864e-03\n",
      "  -3.71888367e-03 -4.83066330e-03  3.38370209e-03 -2.63570136e-03\n",
      "   7.69141416e-03  1.79373270e-03 -3.69529650e-04 -6.59886118e-04\n",
      "   6.48609075e-04  7.42161464e-03 -2.98408806e-03  1.69286056e-03\n",
      "   1.63019258e-03  4.17414911e-03  2.56682893e-03 -2.16102308e-04\n",
      "   4.11689461e-04  7.74741617e-03  4.58231528e-04 -2.89118257e-03\n",
      "  -1.08175415e-04  2.08797296e-03  7.62271866e-04 -1.35031150e-03\n",
      "  -3.38332050e-04 -1.19144386e-03  4.55969273e-03  2.28458247e-03\n",
      "   1.31618204e-02 -3.43904270e-03  4.54689600e-03 -6.02568935e-04\n",
      "  -6.31355651e-04  5.32734072e-03  7.54781452e-03 -2.07897345e-03\n",
      "   6.29476121e-04  2.84572139e-03  1.62377995e-03 -3.85033472e-04\n",
      "   7.46483304e-03 -3.85678542e-03 -7.49800711e-03  2.03830492e-02\n",
      "  -1.03049852e-02 -4.63616213e-03  3.66336640e-03  5.58890404e-03\n",
      "   3.35548699e-03 -2.21589292e-03  1.73374748e-02  1.37019813e-03\n",
      "   4.30970451e-03  4.28494416e-04  2.47123863e-03  7.46670069e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'хуже уже некуда. \\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer('как дела?', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d99a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
